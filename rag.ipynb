{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[Document(metadata={'source': './bella_vista.txt'}, page_content=\"Q: What are the hours of operation for Bella Vista?\\nA: Bella Vista is open from 11 a.m. to 11 p.m. from Monday to Saturday. On Sundays, we welcome guests from 12 p.m. to 10 p.m.\\n\\nQ: What type of cuisine does Bella Vista serve?\\nA: Bella Vista offers a delightful blend of Mediterranean and contemporary American cuisine. We pride ourselves on using the freshest ingredients, many of which are sourced locally.\\n\\nQ: Do you offer vegetarian or vegan options at Bella Vista?\\nA: Absolutely! Bella Vista boasts a diverse menu that includes a variety of vegetarian and vegan dishes. Our chefs are also happy to customize dishes based on dietary needs.\\n\\nQ: Is Bella Vista family-friendly?\\nA: Yes, Bella Vista is a family-friendly establishment. We have a dedicated kids' menu and offer high chairs and booster seats for our younger guests.\\n\\nQ: Can I book private events at Bella Vista?\\nA: Certainly! Bella Vista has a private dining area perfect for events, parties, or corporate gatherings. We also offer catering services for off-site events.\\n\\nQ: What's the ambiance like at Bella Vista?\\nA: Bella Vista boasts a cozy and elegant setting, with ambient lighting, comfortable seating, and a stunning view of the city skyline. Whether you're looking for a romantic dinner or a casual meal with friends, Bella Vista provides the perfect atmosphere.\\n\\nQ: Do I need a reservation for Bella Vista?\\nA: While walk-ins are always welcome, we recommend making a reservation, especially during weekends and holidays, to ensure a seamless dining experience.\")]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "print(load_dotenv(find_dotenv()))\n",
    "\n",
    "\n",
    "loader = TextLoader(\"./bella_vista.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "print(docs)\n",
    "print(len(docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Output:\n",
      " ### Summary\n",
      "\n",
      "The paper examines Retrieval-Augmented Generation (RAG) techniques, which enhance large language models (LLMs) by integrating retrieval mechanisms to incorporate up-to-date information, mitigate hallucinations, and improve response quality. RAG workflows are complex, involving multiple components like query classification, retrieval, reranking, repacking, and summarization, each with various implementation options. The study conducts extensive experiments to identify optimal RAG practices, focusing on balancing performance and efficiency. It also explores multimodal retrieval techniques to enhance question-answering capabilities for visual inputs. The authors propose strategies for deploying RAG systems effectively and provide resources for further exploration.\n",
      "\n",
      "### Key Findings\n",
      "\n",
      "1. **Complexity of RAG Workflows**: RAG involves multiple components with various implementation options, making it complex and challenging to optimize.\n",
      "\n",
      "2. **Optimal Practices for RAG**: The study identifies optimal practices for RAG through a systematic evaluation of existing methods and their combinations.\n",
      "\n",
      "3. **Multimodal Enhancements**: Introducing multimodal retrieval techniques significantly improves question-answering capabilities for visual inputs and accelerates multimodal content generation.\n",
      "\n",
      "4. **Performance vs. Efficiency**: The research suggests strategies to balance performance and efficiency in RAG deployments, emphasizing the trade-offs involved.\n",
      "\n",
      "5. **Comprehensive Evaluation Framework**: The study introduces a robust framework for evaluating RAG models, covering general, specialized, and RAG-specific capabilities.\n",
      "\n",
      "6. **Contribution of Individual Modules**: Each module in the RAG workflow contributes uniquely to overall system performance, with specific recommendations for query classification, retrieval, reranking, repacking, and summarization.\n",
      "\n",
      "7. **Importance of Reranking**: Reranking significantly enhances the relevance of retrieved documents, impacting the quality of generated responses.\n",
      "\n",
      "8. **Fine-Tuning Insights**: Fine-tuning the generator with a mix of relevant and random contexts during training enhances robustness and context utilization.\n",
      "\n",
      "9. **Two Distinct Implementation Strategies**: The paper proposes two RAG implementation strategies, focusing on either maximizing performance or balancing efficiency and efficacy.\n",
      "\n",
      "10. **Potential for Multimodal Expansion**: The study suggests extending RAG to other modalities like video and speech, highlighting the potential of cross-modal retrieval techniques.\n",
      "\n",
      "Final Summary of the Document:\n",
      "The paper examines Retrieval-Augmented Generation (RAG) techniques, which enhance large language models (LLMs) by integrating retrieval mechanisms to incorporate up-to-date information, mitigate hallucinations, and improve response quality. RAG workflows are complex, involving multiple components like query classification, retrieval, reranking, repacking, and summarization, each with various implementation options. The study conducts extensive experiments to identify optimal RAG practices, focusing on balancing performance and efficiency. It also explores multimodal retrieval techniques to enhance question-answering capabilities for visual inputs. The authors propose strategies for deploying RAG systems effectively and provide resources for further exploration.\n",
      "\n",
      "Final Key Findings of the Document:\n",
      "1. **Complexity of RAG Workflows**: RAG involves multiple components with various implementation options, making it complex and challenging to optimize.  2. **Optimal Practices for RAG**: The study identifies optimal practices for RAG through a systematic evaluation of existing methods and their combinations.  3. **Multimodal Enhancements**: Introducing multimodal retrieval techniques significantly improves question-answering capabilities for visual inputs and accelerates multimodal content generation.  4. **Performance vs. Efficiency**: The research suggests strategies to balance performance and efficiency in RAG deployments, emphasizing the trade-offs involved.  5. **Comprehensive Evaluation Framework**: The study introduces a robust framework for evaluating RAG models, covering general, specialized, and RAG-specific capabilities.  6. **Contribution of Individual Modules**: Each module in the RAG workflow contributes uniquely to overall system performance, with specific recommendations for query classification, retrieval, reranking, repacking, and summarization.  7. **Importance of Reranking**: Reranking significantly enhances the relevance of retrieved documents, impacting the quality of generated responses.  8. **Fine-Tuning Insights**: Fine-tuning the generator with a mix of relevant and random contexts during training enhances robustness and context utilization.  9. **Two Distinct Implementation Strategies**: The paper proposes two RAG implementation strategies, focusing on either maximizing performance or balancing efficiency and efficacy.  10. **Potential for Multimodal Expansion**: The study suggests extending RAG to other modalities like video and speech, highlighting the potential of cross-modal retrieval techniques.\n",
      "\n",
      "Processed Documents with Updated Metadata:\n",
      "Document 1:\n",
      "Content: Searching for Best Practices in Retrieval-Augmented\n",
      "Generation\n",
      "Xiaohua Wang, Zhenghua Wang, Xuan Gao...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. Retrieval-Augmented Generation (RAG)\\n2. Best Practices\\n3. Large Language Models\\n4. Information Retrieval\\n5. Query-Dependent Retrievals\\n6. Multimodal Retrieval Techniques\\n7. Question-Answering\\n8. Hallucination Mitigation\\n9. Response Quality\\n10. Specialized Domains\\n11. Efficiency in RAG\\n12. Multimodal Content Generation\\n13. Machine Learning\\n14. Artificial Intelligence\\n15. Computational Efficiency\\n16. Fudan University\\n17. Intelligent Information Processing']}\n",
      "\n",
      "\n",
      "Document 2:\n",
      "Content: question-answering capabilities about visual inputs and accelerate the generation\n",
      "of multimodal cont...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['Here is a list of relevant tags for the provided text:\\n\\n1. Question-Answering\\n2. Visual Inputs\\n3. Multimodal Content\\n4. Retrieval as Generation\\n5. Generative Language Models\\n6. Outdated Information\\n7. Fabricated Facts\\n8. Reinforcement Learning\\n9. Retrieval-Augmented Generation (RAG)\\n10. Pretraining\\n11. Retrieval-Based Models\\n12. Model Performance\\n13. Query-Dependent Retrieval\\n14. Query Classification\\n15. Document Retrieval\\n16. Document Reranking\\n17. Document Repacking\\n18. Summarization\\n19. Large Language Models (LLMs)\\n20. GitHub Resource']}\n",
      "\n",
      "\n",
      "Document 3:\n",
      "Content: order of retrieved documents based on their relevance to the query), repacking (organizing the\n",
      "retri...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. Retrieval-Augmented Generation (RAG)\\n2. Document Relevance\\n3. Repacking\\n4. Summarization\\n5. Document Chunking\\n6. Embeddings\\n7. Query Classification\\n8. Reranking\\n9. Large Language Models (LLM)\\n10. Query Rewriting\\n11. Query Decomposition\\n12. Hybrid Search\\n13. Extractive Summarization\\n14. Abstractive Summarization\\n15. Vector Database\\n16. BM25\\n17. monoT5\\n18. monoBERT\\n19. RankLLaMA\\n20. TILDE\\n21. Milvus\\n22. Faiss\\n23. Weaviate\\n24. Qdrant\\n25. Chroma\\n26. LLM-Embedder\\n27. intfloat/e5\\n28. BAAI/bge\\n29. Jina-embeddings-v2\\n30. Gte\\n31. all-mpnet-base-v2\\n32. Sliding Windows\\n33. Chunk Metadata\\n34. General Performance Evaluation\\n35. Specific Domains Evaluation\\n36. Retrieval Capability Evaluation\\n37. Fine-tuning\\n38. Disturbance Testing']}\n",
      "\n",
      "\n",
      "Document 4:\n",
      "Content: Retrieval Capability• \n",
      "• \n",
      "•  \n",
      "Fine -tune\n",
      "Disturb\n",
      "Random\n",
      "Normal• \n",
      "• \n",
      "•    \n",
      "Retrieval SourceFigure 1: ...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['Here is a list of relevant tags for the provided text:\\n\\n1. Retrieval-Augmented Generation (RAG)\\n2. Fine-Tuning\\n3. Vector Databases\\n4. Query Rewriting\\n5. Pseudo-Responses\\n6. Embedding Models\\n7. Contrastive Learning\\n8. Feature Representations\\n9. Document Retrieval\\n10. Machine Learning Workflow\\n11. Experimentation\\n12. Optimal Practices\\n13. System Efficiency\\n14. Processing Variability\\n15. Information Retrieval']}\n",
      "\n",
      "\n",
      "Document 5:\n",
      "Content: positive and negative query-response pairs [ 11,12]. The techniques chosen for each step and their\n",
      "c...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. RAG (Retrieval-Augmented Generation)\\n2. Query-response pairs\\n3. Workflow optimization\\n4. Experimental methodology\\n5. Performance evaluation\\n6. Efficiency vs. performance\\n7. Best practices identification\\n8. Systematic study\\n9. Module comparison\\n10. Method selection\\n11. Application scenarios\\n12. Strategy suggestions']}\n",
      "\n",
      "\n",
      "Document 6:\n",
      "Content: several strategies for deploying RAG that balance both performance and efficiency.\n",
      "The contributions...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. Retrieval-Augmented Generation (RAG)\\n2. Performance Optimization\\n3. Efficiency\\n4. Evaluation Metrics\\n5. Multimodal Retrieval\\n6. Question-Answering\\n7. Large Language Models (LLMs)\\n8. ChatGPT\\n9. LLaMA\\n10. Hallucinations\\n11. Knowledge-Intensive Tasks\\n12. Domain-Specific Context\\n13. Query Transformation\\n14. Retrieval Transformation\\n15. Retriever Fine-Tuning\\n16. Generator Fine-Tuning']}\n",
      "\n",
      "\n",
      "Document 7:\n",
      "Content: LLMs [ 6]. Previous works have optimized the RAG pipeline through query and retrieval transfor-\n",
      "mati...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. Large Language Models (LLMs)\\n2. RAG Pipeline Optimization\\n3. Query Transformation\\n4. Retrieval Enhancement\\n5. Retriever Fine-tuning\\n6. Generator Fine-tuning\\n7. Query2Doc\\n8. HyDE\\n9. TOC (Query Decomposition)\\n10. LlamaIndex\\n11. Pseudo-document Generation\\n12. Pseudo-query Generation\\n13. Contrastive Learning\\n14. Semantic Embeddings\\n15. Document Post-processing\\n16. Hierarchical Prompt Summarization\\n17. Abstractive Compression\\n18. Extractive Compression\\n19. Context Length Reduction\\n20. Redundancy Removal']}\n",
      "\n",
      "\n",
      "Document 8:\n",
      "Content: output, with techniques like hierarchical prompt summarization [ 22] and using abstractive and\n",
      "extra...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. Hierarchical prompt summarization\\n2. Abstractive compression\\n3. Extractive compression\\n4. Context length reduction\\n5. Redundancy removal\\n6. Retriever enhancement\\n7. Document chunking\\n8. Embedding methods\\n9. Retrieval performance\\n10. Chunk length optimization\\n11. LlamaIndex\\n12. Small2Big method\\n13. Sliding window method\\n14. Reranking\\n15. Deep language models\\n16. BERT\\n17. T5\\n18. LLaMA\\n19. Slow inference\\n20. TILDE\\n21. Query likelihood precomputation\\n22. Document ranking\\n23. Fine-tuning\\n24. RAG framework\\n25. Retriever optimization\\n26. Generator optimization\\n27. Faithful content generation\\n28. Robust content generation\\n29. Integrated system fine-tuning']}\n",
      "\n",
      "\n",
      "Document 9:\n",
      "Content: passages for the generator [ 33–35]. Holistic approaches treat RAG as an integrated system, fine-tun...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. Retrieval-Augmented Generation (RAG)\\n2. Holistic Approaches\\n3. Fine-Tuning\\n4. Text Generation\\n5. Large Language Models (LLMs)\\n6. Multimodal Systems\\n7. AI-Generated Content\\n8. Surveys and Methodologies\\n9. French Translation\\n10. Background Knowledge\\n11. Planning\\n12. Role-Playing\\n13. Geography of Europe\\n14. Rainfall Changes in Europe\\n15. Computer Science\\n16. System Malfunction']}\n",
      "\n",
      "\n",
      "Document 10:\n",
      "Content: Write an article about the geography of Europe, focusing \n",
      "on the changes in rainfall in the western ...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. Geography\\n2. Europe\\n3. Western Europe\\n4. Rainfall\\n5. Climate Change\\n6. Environmental Changes\\n7. Weather Patterns\\n8. Article Writing\\n9. Meteorology\\n10. Geography of Europe']}\n",
      "\n",
      "\n",
      "Document 11:\n",
      "Content: should I drive or take a plane?           <  Decision making  >\n",
      "I had a quarrel with my parents beca...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['Here is a list of relevant tags for the provided text:\\n\\n1. Decision Making\\n2. Relationship Advice\\n3. Family Conflict\\n4. Persuasion\\n5. Retrieval-Augmented Generation (RAG)\\n6. Machine Learning Workflow\\n7. Algorithm Implementation\\n8. Query Classification\\n9. Natural Language Processing (NLP)\\n10. Large Language Models (LLMs)\\n11. Information Retrieval\\n12. Best Practices\\n13. Experimental Setup']}\n",
      "\n",
      "\n",
      "Document 12:\n",
      "Content: time. Therefore, we begin by classifying queries to determine the necessity of retrieval. Queries\n",
      "re...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['Here is a list of relevant tags for the provided text:\\n\\n1. Query Classification\\n2. Retrieval-Augmented Generation (RAG)\\n3. Large Language Models (LLMs)\\n4. Information Retrieval\\n5. Natural Language Processing (NLP)\\n6. Machine Learning\\n7. BERT-base-multilingual\\n8. Model Performance Metrics\\n9. Sufficient vs. Insufficient Information\\n10. Automation\\n11. Experimental Details\\n12. Workflow Optimization\\n13. Embedding Models\\n14. Multilingual Models\\n15. Query Processing']}\n",
      "\n",
      "\n",
      "Document 13:\n",
      "Content: classification.\n",
      "4Embedding Modelnamespace-Pt/msmarco\n",
      "MRR@1 MRR@10 MRR@100 R@1 R@10 R@100\n",
      "BAAI/LLM-Em...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['Here is a list of relevant tags for the provided text:\\n\\n1. Embedding Models\\n2. Model Performance\\n3. MRR (Mean Reciprocal Rank)\\n4. Recall (R)\\n5. BAAI\\n6. Alibaba-NLP\\n7. Sentence-Transformers\\n8. JinaAI\\n9. Intfloat\\n10. Model Comparison\\n11. NLP (Natural Language Processing)\\n12. Information Retrieval\\n13. Chunking\\n14. Document Segmentation\\n15. Token-level Chunking\\n16. Namespace-Pt/msmarco\\n17. Retrieval Precision\\n18. LLMs (Large Language Models)\\n19. Semantic Analysis\\n20. Performance Metrics']}\n",
      "\n",
      "\n",
      "Document 14:\n",
      "Content: sentence, and semantic levels.\n",
      "•Token-level Chunking is straightforward but may split sentences, aff...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. Chunking\\n2. Token-level Chunking\\n3. Sentence-level Chunking\\n4. Semantic-level Chunking\\n5. Chunk Size\\n6. Performance Metrics\\n7. Faithfulness\\n8. Relevancy\\n9. Text Comprehension\\n10. Retrieval Quality\\n11. Context Preservation\\n12. LLMs (Large Language Models)\\n13. LlamaIndex\\n14. Embedding Models\\n15. text-embedding-ada-0022\\n16. zephyr-7b-alpha3\\n17. gpt-3.5-turbo\\n18. Chunk Overlap\\n19. Evaluation Module']}\n",
      "\n",
      "\n",
      "Document 15:\n",
      "Content: which supports long input length. We choose\n",
      "zephyr-7b-alpha3and gpt-3.5-turbo4as\n",
      "generation model an...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. Long input length\\n2. Zephyr-7b-alpha\\n3. GPT-3.5-turbo\\n4. Chunk overlap\\n5. Document analysis\\n6. Query generation\\n7. Chunk size impact\\n8. Advanced chunking techniques\\n9. Small-to-big technique\\n10. Sliding window technique\\n11. Retrieval quality\\n12. LLM-Embedder\\n13. Embedding model\\n14. Semantic matching\\n15. Contextual information\\n16. Token size\\n17. Corpus analysis\\n18. Information retrieval']}\n",
      "\n",
      "\n",
      "Document 16:\n",
      "Content: results are shown in Table 4.\n",
      "3.2.3 Embedding Model Selection\n",
      "Choosing the right embedding model is ...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. Embedding Model\\n2. Semantic Matching\\n3. Query Processing\\n4. FlagEmbedding\\n5. Dataset\\n6. Chunk Skilllyft_2021\\n7. Faithfulness\\n8. Relevancy\\n9. Vector Databases\\n10. Metadata Addition\\n11. Retrieval Methods\\n12. Indexing\\n13. Approximate Nearest Neighbor (ANN)\\n14. LLM-Embedder\\n15. BAAI/bge-large-en\\n16. Performance Evaluation\\n17. Open Source\\n18. Semantic Search\\n19. Information Retrieval']}\n",
      "\n",
      "\n",
      "Document 17:\n",
      "Content: uments relevant to queries through various indexing and approximate nearest neighbor (ANN)\n",
      "methods.\n",
      "...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['Here is a list of relevant tags for the provided text:\\n\\n1. Vector Database\\n2. Indexing\\n3. Approximate Nearest Neighbor (ANN)\\n4. Database Evaluation\\n5. Cloud-native\\n6. Billion-scale\\n7. Hybrid Search\\n8. Scalability\\n9. Open-source\\n10. Weaviate\\n11. Faiss\\n12. Chroma\\n13. Qdrant\\n14. Milvus\\n15. Large Language Models (LLM)\\n16. Data Flexibility\\n17. Search Optimization\\n18. Cloud Integration\\n19. Dataset Management']}\n",
      "\n",
      "\n",
      "Document 18:\n",
      "Content: databases evaluated, meeting all the essential criteria and outperforming other open-source options....\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['Here are some relevant tags for the provided text:\\n\\n1. Information Retrieval\\n2. Open-source Databases\\n3. Performance Evaluation\\n4. TREC DL19\\n5. TREC DL20\\n6. Unsupervised Learning\\n7. Supervised Learning\\n8. BM25\\n9. Contriever\\n10. LLM-Embedder\\n11. Query Rewriting\\n12. Query Decomposition\\n13. Hybrid Search\\n14. HyDE\\n15. GitHub\\n16. Hugging Face\\n17. Metrics (mAP', 'nDCG@10', 'R@50', 'R@1k', 'Latency)\\n18. Pseudo-document\\n19. Dataset Evaluation\\n20. Retrieval Methods\\n\\nThese tags capture the key aspects and methodologies discussed in the text.']}\n",
      "\n",
      "\n",
      "Document 19:\n",
      "Content: w/ 1 pseudo-doc + query 50.87 75.44 54 .93 88.76 7.21 50.94 73.94 63.80 88 .03 2 .14\n",
      "w/ 8 pseudo-doc...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['Here is a list of relevant tags for the provided text:\\n\\n1. Information Retrieval\\n2. Query Transformation\\n3. Pseudo-documents\\n4. Query Rewriting\\n5. Query Decomposition\\n6. HyDE\\n7. LLM-Embedder\\n8. Document Retrieval\\n9. Semantic Information\\n10. Query Enhancement\\n11. Similarity Matching\\n12. Rewrite-Retrieve-Read Framework\\n13. Retrieval Methods\\n14. Hypothetical Documents\\n15. Embedding Techniques']}\n",
      "\n",
      "\n",
      "Document 20:\n",
      "Content: user query and uses the embedding of hypothetical answers to retrieve similar documents. One\n",
      "notable...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. Information Retrieval\\n2. Dense Retrieval\\n3. Sparse Retrieval\\n4. Hybrid Search\\n5. BM25\\n6. Contriever\\n7. HyDE\\n8. Lexical Search\\n9. Vector Search\\n10. Embedding\\n11. Supervised Methods\\n12. Unsupervised Methods\\n13. Query Rewriting\\n14. Query Decomposition\\n15. Performance Evaluation\\n16. Latency\\n17. TREC DL 2019\\n18. TREC DL 2020\\n19. Passage Ranking\\n20. Hypothetical Answers\\n21. Document Concatenation\\n22. Pseudo-documents']}\n",
      "\n",
      "\n",
      "Document 21:\n",
      "Content: Table 7 shows the impact of different concatenation strategies for hypothetical documents and querie...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. Information Retrieval\\n2. Concatenation Strategies\\n3. HyDE\\n4. Pseudo-documents\\n5. Query Processing\\n6. Hybrid Search\\n7. Hyperparameter Tuning\\n8. TREC DL19\\n9. TREC DL20\\n10. Performance Metrics\\n11. mAP (Mean Average Precision)\\n12. nDCG@10 (Normalized Discounted Cumulative Gain at 10)\\n13. R@50 (Recall at 50)\\n14. R@1k (Recall at 1000)\\n15. Latency\\n16. MS MARCO Passage Ranking\\n17. Reranking\\n18. Random Ordering\\n19. BM25\\n20. DLM Reranking\\n21. monoT5\\n22. monoBERT\\n23. RankLLaMA\\n24. TILDE\\n25. Trade-off Analysis']}\n",
      "\n",
      "\n",
      "Document 22:\n",
      "Content: measured in seconds per query.\n",
      "enhance retrieval performance, though at the cost of increased latenc...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. Information Retrieval\\n2. Hybrid Search\\n3. Sparse Retrieval\\n4. Dense Retrieval\\n5. Latency\\n6. Retrieval Performance\\n7. Relevance Score\\n8. α Value (Alpha)\\n9. Reranking Methods\\n10. Trade-off Analysis\\n11. Retrieval Effectiveness\\n12. Efficiency\\n13. Performance Evaluation\\n14. Hypothetical Documents\\n15. Query Processing']}\n",
      "\n",
      "\n",
      "Document 23:\n",
      "Content: documents, ensuring that the most pertinent information appears at the top of the list. This phase u...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. Reranking\\n2. DLM Reranking\\n3. TILDE Reranking\\n4. Deep Language Models (DLMs)\\n5. Query Likelihood\\n6. Document Relevancy\\n7. Query Similarity\\n8. Classification\\n9. Performance\\n10. Efficiency\\n11. Fine-tuning\\n12. Token Probability\\n13. Extractive Method\\n14. BM25\\n15. Contriever\\n16. Recomp\\n17. Abstractive Method\\n18. SelectiveContext\\n19. LongLLMlingua\\n20. Information Retrieval']}\n",
      "\n",
      "\n",
      "Document 24:\n",
      "Content: Recomp (extractive) 27.84 34 35 .32 60 29 .46 58 30 .87 51\n",
      "Abstractive Method\n",
      "SelectiveContext 25.05...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. Summarization Methods\\n2. Extractive Summarization\\n3. Abstractive Summarization\\n4. SelectiveContext\\n5. LongLLMlingua\\n6. Recomp\\n7. TILDEv2\\n8. MS MARCO Passage Ranking\\n9. PyGaggle\\n10. monoT5\\n11. monoBERT\\n12. RankLLaMA\\n13. Reranking\\n14. Document Indexing\\n15. NCE Loss\\n16. Efficiency\\n17. Performance\\n18. Document Repacking\\n19. LLM Response Generation\\n20. Experimental Setup']}\n",
      "\n",
      "\n",
      "Document 25:\n",
      "Content: The performance of subsequent processes, such as LLM response generation, may be affected by the\n",
      "ord...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['Here is a list of relevant tags for the provided text:\\n\\n1. LLM (Large Language Model)\\n2. Response Generation\\n3. Document Reordering\\n4. Repacking\\n5. Forward Method\\n6. Reverse Method\\n7. Sides Method\\n8. Reranking\\n9. Summarization\\n10. Extractive Summarization\\n11. Abstractive Summarization\\n12. Information Retrieval\\n13. RAG Pipeline (Retrieval-Augmented Generation)\\n14. Document Relevance\\n15. Workflow Optimization\\n16. Liu et al. [48]\\n17. Text Compression\\n18. Inference Process\\n\\nThese tags capture the key concepts and processes discussed in the text.']}\n",
      "\n",
      "\n",
      "Document 26:\n",
      "Content: tences, then score and rank them based on importance. Abstractive compressors synthesize infor-\n",
      "mati...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. Query-based summarization\\n2. Non-query-based summarization\\n3. Abstractive compression\\n4. Extractive compression\\n5. RAG (Retrieval-Augmented Generation)\\n6. Recomp\\n7. LongLLMLingua\\n8. Selective Context\\n9. Information synthesis\\n10. Lexical units informativeness\\n11. Causal language model\\n12. Benchmark datasets\\n13. NQ (Natural Questions)\\n14. TriviaQA\\n15. HotpotQA\\n16. Comparative analysis\\n17. Summarization performance\\n18. Generalization capabilities']}\n",
      "\n",
      "\n",
      "Document 27:\n",
      "Content: it as an alternative method. Additional implementation details and discussions on non-query-based\n",
      "me...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['Here is a list of relevant tags for the provided text:\\n\\n1. Fine-tuning\\n2. Generator\\n3. Non-query-based methods\\n4. Contextual impact\\n5. Query-relevant documents\\n6. RAG system\\n7. Negative log-likelihood\\n8. Context composition\\n9. Model training\\n10. Augmented context\\n11. QA (Question Answering)\\n12. Language Model (LM)']}\n",
      "\n",
      "\n",
      "Document 28:\n",
      "Content: We denote the base LM generator not fine-tuned as Mb, and the model fine-tuned under the\n",
      "correspondi...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. Language Model\\n2. Fine-tuning\\n3. QA (Question Answering)\\n4. Reading Comprehension\\n5. Evaluation Metrics\\n6. Llama-2-7B\\n7. Validation Sets\\n8. Inference\\n9. Ground-truth Coverage\\n10. Model Training\\n11. Dataset\\n12. Generator\\n13. Results\\n14. Random Documents\\n15. Machine Learning']}\n",
      "\n",
      "\n",
      "Document 29:\n",
      "Content: withDg,Dr,Dgr, andD∅, where D∅indicates\n",
      "inference without retrieval. Figure 3 presents\n",
      "our main resu...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['Here is a list of relevant tags based on the provided text:\\n\\n1. Machine Learning\\n2. Natural Language Processing (NLP)\\n3. Retrieval-Augmented Generation (RAG)\\n4. Model Training\\n5. Contextual Embedding\\n6. Document Retrieval\\n7. Data Augmentation\\n8. Robustness\\n9. Inference\\n10. Experimental Results\\n11. Hyperparameters\\n12. Optimizing Models\\n13. Summarization\\n14. Llama2-7B-Chat Model\\n15. Commonsense Reasoning\\n16. Fact Checking\\n17. Open-Domain Question Answering (ODQA)\\n18. Multihop Reasoning\\n19. Medical Applications\\n20. Performance Metrics (Accuracy', 'F1 Score', 'EM)\\n21. Latency\\n22. Hybrid Models\\n23. monoT5\\n24. Recomp\\n\\nThese tags capture the key concepts and methodologies discussed in the text.']}\n",
      "\n",
      "\n",
      "Document 30:\n",
      "Content: 10MethodCommonsense Fact Check ODQA Multihop Medical RAG Avg.\n",
      "Acc Acc EM F1 EM F1 Acc Score Score F1...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. Commonsense\\n2. Fact Check\\n3. ODQA (Open-Domain Question Answering)\\n4. Multihop\\n5. Medical\\n6. RAG (Retrieval-Augmented Generation)\\n7. Classification Module\\n8. Hybrid Model\\n9. HyDE (Hybrid Dense Encoder)\\n10. monoT5\\n11. Recomp\\n12. Retrieval Module\\n13. Reranking Module\\n14. Repacking Module\\n15. monoBERT\\n16. RankLLaMA\\n17. TILDEv2\\n18. Latency\\n19. Accuracy\\n20. F1 Score\\n21. EM (Exact Match)\\n22. Sides\\n23. Forward\\n24. Reverse']}\n",
      "\n",
      "\n",
      "Document 31:\n",
      "Content: + forward 0.722 0.599 0.379 0.437 0.215 0.260 0.472 0.542 0.474 0.349 11.68\n",
      "+reverse 0.728 0.592 0.3...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. RAG (Retrieval-Augmented Generation)\\n2. Hybrid with HyDE\\n3. monoT5\\n4. Summarization Module\\n5. Recomp\\n6. LongLLMLingua\\n7. Vector Database\\n8. Milvus\\n9. English Wikipedia\\n10. Medical Data\\n11. Query Classification\\n12. Reranking\\n13. Commonsense Reasoning\\n14. Fact Checking\\n15. Open-Domain QA\\n16. MultiHop QA\\n17. Medical QA\\n18. NLP (Natural Language Processing)\\n19. Performance Evaluation\\n20. Latency Measurement']}\n",
      "\n",
      "\n",
      "Document 32:\n",
      "Content: mance of RAG systems. Specifically: (I) Commonsense Reasoning ; (II) Fact Checking ; (III)\n",
      "Open-Doma...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. RAG Systems\\n2. Commonsense Reasoning\\n3. Fact Checking\\n4. Open-Domain QA\\n5. MultiHop QA\\n6. Medical QA\\n7. RAG Capabilities\\n8. Evaluation Metrics\\n9. Faithfulness\\n10. Context Relevancy\\n11. Answer Relevancy\\n12. Answer Correctness\\n13. Retrieval Similarity\\n14. Cosine Similarity\\n15. Accuracy\\n16. Token-level F1 Score\\n17. Exact Match (EM) Score\\n18. Dataset Sub-sampling\\n19. Query Classification Module\\n20. Retrieval Module\\n21. Hybrid with HyDE Method']}\n",
      "\n",
      "\n",
      "Document 33:\n",
      "Content: reduction in latency time from 16.41to11.58seconds per query.\n",
      "11•Retrieval Module: While the “Hybrid...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. Latency Reduction\\n2. Query Processing\\n3. Retrieval Module\\n4. Hybrid Method\\n5. RAG Score\\n6. Computational Cost\\n7. Reranking Module\\n8. MonoT5\\n9. Document Relevance\\n10. Repacking Module\\n11. Reverse Configuration\\n12. Context Positioning\\n13. Summarization Module\\n14. Performance Optimization\\n15. Response Time\\n16. Module Contribution\\n17. Experimental Results']}\n",
      "\n",
      "\n",
      "Document 34:\n",
      "Content: response time.\n",
      "The experimental results demonstrate that each module contributes uniquely to the ove...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['the summarization module. This setup achieved a slightly lower average score of 0.465 but with a more efficient computational process.\\n\\nBased on the text provided', 'here is a list of relevant tags:\\n\\n1. RAG system\\n2. query classification\\n3. retrieval\\n4. reranking\\n5. repacking\\n6. summarization\\n7. performance\\n8. latency\\n9. efficiency\\n10. accuracy\\n11. best practices\\n12. experimental results\\n13. Hybrid with HyDE\\n14. monoT5\\n15. Reverse\\n16. Recomp\\n17. TILDEv2\\n18. computational process']}\n",
      "\n",
      "\n",
      "Document 35:\n",
      "Content: it is recommended to incorporate the query classification module, implement the Hybrid method\n",
      "for re...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['Here is a list of relevant tags for the given text:\\n\\n1. Query Classification\\n2. Hybrid Retrieval Method\\n3. TILDEv2 Reranking\\n4. Reverse Repacking\\n5. Recomp Summarization\\n6. Latency Reduction\\n7. Multimodal Extension\\n8. RAG (Retrieval-Augmented Generation)\\n9. Text2Image Retrieval\\n10. Image2Text Retrieval\\n11. Multimodal Retrieval\\n12. Image Generation\\n13. Groundedness\\n14. Efficiency\\n15. Information Retrieval\\n16. System Optimization\\n17. Processing Time\\n18. Performance Preservation']}\n",
      "\n",
      "\n",
      "Document 36:\n",
      "Content: new content, which can occasionally result in factual errors or inaccuracies.\n",
      "•Efficiency : Retrieva...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. Multimodal Retrieval\\n2. Text-to-Image Retrieval\\n3. Image-to-Text Retrieval\\n4. Efficiency\\n5. Factual Errors\\n6. Inaccuracies\\n7. Image Generation\\n8. Image Captioning\\n9. Computational Resources\\n10. Similarity Matching\\n11. User Query\\n12. Maintainability\\n13. Image Database\\n14. Model Fine-Tuning']}\n",
      "\n",
      "\n",
      "Document 37:\n",
      "Content: •Maintainability : Generation models often necessitate careful fine-tuning to tailor them for new\n",
      "ap...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['design', 'allowing for independent optimization of each component within the retrieval-augmented generation framework.\\n\\n1. Retrieval-Augmented Generation (RAG)\\n2. Large Language Models (LLM)\\n3. Fine-Tuning\\n4. Cross-Modal Retrieval\\n5. Video and Speech Modalities\\n6. Content Quality and Reliability\\n7. Evaluation Benchmark\\n8. Modular Design\\n9. Joint Training\\n10. Future Research Directions']}\n",
      "\n",
      "\n",
      "Document 38:\n",
      "Content: have demonstrated the feasibility of training both the retriever and generator jointly. We would\n",
      "lik...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. Joint Training\\n2. Retriever and Generator\\n3. Modular Design\\n4. RAG (Retrieval-Augmented Generation)\\n5. Vector Databases\\n6. Chunking Techniques\\n7. NLP (Natural Language Processing)\\n8. Image Generation\\n9. Speech Modality\\n10. Video Modality\\n11. Future Research Directions\\n12. National Natural Science Foundation of China\\n13. Cross-Modality Expansion\\n14. Experimentation Costs\\n15. Evaluation Techniques']}\n",
      "\n",
      "\n",
      "Document 39:\n",
      "Content: Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano,\n",
      "Jan Leike...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['- Language Models\\n- Human Feedback\\n- Neural Information Processing Systems (NeurIPS)\\n- Direct Preference Optimization\\n- Sequence Likelihood Calibration\\n- Rank Responses\\n- Alignment with Human Preferences\\n- Representation Engineering\\n- Retrieval-Augmented Generation\\n- Conference Proceedings\\n- ArXiv Preprints\\n- Artificial Intelligence\\n- Machine Learning\\n- Large Language Models']}\n",
      "\n",
      "\n",
      "Document 40:\n",
      "Content: preprint arXiv:2312.10997 , 2023.\n",
      "[7]Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and Lemao Liu. A sur...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. Retrieval-Augmented Text Generation\\n2. Query Rewriting\\n3. Large Language Models\\n4. Dense Retrieval\\n5. Contrastive Pre-training\\n6. Text Embeddings\\n7. Weakly-Supervised Learning\\n8. Zero-Shot Learning\\n9. Chinese Embedding\\n10. GPT-4\\n11. LLaMA Model\\n12. Information Retrieval\\n13. Preprint\\n14. arXiv Publications\\n15. ACM SIGIR Conference']}\n",
      "\n",
      "\n",
      "Document 41:\n",
      "Content: [14] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\n",
      "thée ...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. Large Language Models (LLMs)\\n2. Hallucination Detection\\n3. Bayesian Sequential Estimation\\n4. Query Expansion\\n5. Retrieval-Augmented Models\\n6. Foundation Models\\n7. Generative Models\\n8. Ambiguous Questions\\n9. AI Survey\\n10. Open Language Models\\n11. LLaMA\\n12. LlamaIndex\\n13. arXiv Preprints\\n14. Natural Language Processing (NLP)\\n15. Empirical Methods in NLP']}\n",
      "\n",
      "\n",
      "Document 42:\n",
      "Content: [20] Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie. Retrieve anything to\n",
      "aug...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. Large Language Models\\n2. Retrieval-Augmented Generation\\n3. Text Embeddings\\n4. Contrastive Learning\\n5. Prompt Compression\\n6. Context Filtering\\n7. Document Ranking\\n8. Sequence-to-Sequence Models\\n9. Multi-Stage Retrieval\\n10. BERT\\n11. LLaMA Model\\n12. Machine Learning\\n13. Natural Language Processing\\n14. arXiv Preprints']}\n",
      "\n",
      "\n",
      "Document 43:\n",
      "Content: multi-stage text retrieval. arXiv preprint arXiv:2310.08319 , 2023.\n",
      "[28] Shengyao Zhuang and Guido Z...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. Multi-stage text retrieval\\n2. Passage re-ranking\\n3. Term independent likelihood model\\n4. Contextualized term matching\\n5. Passage expansion\\n6. Search-augmented instruction learning\\n7. Domain-specific language models\\n8. Retrieval-augmented generation (RAG)\\n9. Conversational QA\\n10. Few-shot learning\\n11. Information retrieval\\n12. Natural language processing\\n13. Machine learning\\n14. Language models\\n15. Empirical methods in NLP']}\n",
      "\n",
      "\n",
      "Document 44:\n",
      "Content: Jane A. Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with\n",
      "retrieval aug...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. Few-shot learning\\n2. Retrieval-augmented language models\\n3. Large language models\\n4. Adaptive relevance labeling\\n5. Black-box language models\\n6. Pre-training\\n7. Instruction tuning\\n8. End-to-end generation\\n9. Utility maximization\\n10. Text generation\\n11. Language model survey\\n12. ArXiv\\n13. Research papers\\n14. Computational linguistics']}\n",
      "\n",
      "\n",
      "Document 45:\n",
      "Content: language models. arXiv preprint arXiv:2404.10981 , 2024.\n",
      "[40] Ruochen Zhao, Hailin Chen, Weishi Wang...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. Language Models\\n2. Multimodal Information\\n3. Augmented Generation\\n4. AI-Generated Content\\n5. Retrieval-Augmented Generation (RAG)\\n6. Text Embeddings\\n7. Long Documents\\n8. Semantic Search\\n9. Hybrid Query-Based Retrievers\\n10. Unsupervised Dense Information Retrieval\\n11. Contrastive Learning\\n12. Survey\\n13. arXiv Preprint\\n14. Academic Research\\n15. Information Retrieval']}\n",
      "\n",
      "\n",
      "Document 46:\n",
      "Content: arXiv preprint arXiv:2112.09118 , 2021.\n",
      "[46] Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek S...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. arXiv\\n2. Preprint\\n3. Information Retrieval\\n4. Zero-shot Evaluation\\n5. Machine Reading Comprehension\\n6. Long Contexts\\n7. Language Models\\n8. Prompt Compression\\n9. Computational Linguistics\\n10. AI Benchmarking\\n11. Dataset\\n12. Natural Language Processing (NLP)\\n13. Machine Learning\\n14. Long Context Scenarios\\n15. AI Models']}\n",
      "\n",
      "\n",
      "Document 47:\n",
      "Content: Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S.\n",
      "Hartshorn,...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. LLaMA 2\\n2. Open foundation models\\n3. Fine-tuned chat models\\n4. ArXiv\\n5. Retrieval augmented generation\\n6. Automated evaluation\\n7. European Chapter of the Association for Computational Linguistics\\n8. Multihop questions\\n9. Single-hop question composition\\n10. Transactions of the Association for Computational Linguistics\\n11. Computational linguistics\\n12. Natural language processing\\n13. Machine learning\\n14. Artificial intelligence\\n15. Research papers']}\n",
      "\n",
      "\n",
      "Document 48:\n",
      "Content: for Computational Linguistics , page 539–554, May 2022. doi: 10.1162/tacl_a_00475. URL\n",
      "http://dx.doi...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. Computational Linguistics\\n2. Instruction-tuned LLM\\n3. Deep Learning\\n4. Information Retrieval\\n5. Pyserini\\n6. TREC (Text REtrieval Conference)\\n7. Open Source\\n8. Sparse and Dense Representations\\n9. ACM SIGIR Conference\\n10. ArXiv\\n11. Semantic Scholar\\n12. Reproducibility in Research\\n13. Natural Language Processing (NLP)\\n14. Machine Learning\\n15. Artificial Intelligence (AI)']}\n",
      "\n",
      "\n",
      "Document 49:\n",
      "Content: [57] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh,\n",
      "Chris ...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. Question Answering\\n2. Reading Comprehension\\n3. Natural Language Processing (NLP)\\n4. Machine Learning\\n5. Datasets\\n6. Benchmarking\\n7. Multi-hop Question Answering\\n8. Explainability\\n9. Factoid Questions\\n10. Long-form Answers\\n11. Distant Supervision\\n12. NLP Challenges\\n13. Academic Research\\n14. ArXiv\\n15. Computational Linguistics']}\n",
      "\n",
      "\n",
      "Document 50:\n",
      "Content: [62] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions\n",
      "fo...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['list of relevant tags for the provided text could include:\\n\\n1. Machine Comprehension\\n2. Question Answering\\n3. Natural Language Processing (NLP)\\n4. SQuAD Dataset\\n5. TruthfulQA\\n6. Large Language Models\\n7. Low-rank Adaptation\\n8. Multitask Language Understanding\\n9. AI2 Reasoning Challenge\\n10. Open Book Question Answering\\n11. Empirical Methods in NLP\\n12. Dataset\\n13. Artificial Intelligence\\n14. ArXiv Preprint\\n15. Computational Linguistics']}\n",
      "\n",
      "\n",
      "Document 51:\n",
      "Content: 10.18653/v1/d18-1260. URL http://dx.doi.org/10.18653/v1/d18-1260 .\n",
      "[68] James Thorne, Andreas Vlacho...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['Here is a list of relevant tags based on the provided text:\\n\\n1. Fact Verification\\n2. Large-Scale Datasets\\n3. Natural Language Processing (NLP)\\n4. Semantic Parsing\\n5. Question-Answering (QA)\\n6. Multi-Hop Reasoning\\n7. Language Models\\n8. Biomedical QA\\n9. Compositionality\\n10. Empirical Methods in NLP\\n11. ArXiv\\n12. Semantic Scholar\\n13. Freebase\\n14. Dataset Construction\\n15. Interpretable Models']}\n",
      "\n",
      "\n",
      "Document 52:\n",
      "Content: dataset for biomedical research question answering. In Conference on Empirical Methods in\n",
      "Natural La...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['Here is a list of relevant tags for the provided text:\\n\\n1. Biomedical Research\\n2. Question Answering\\n3. Natural Language Processing (NLP)\\n4. Empirical Methods\\n5. Dataset\\n6. Query Classification\\n7. Retrieval Methods\\n8. BERT\\n9. GPT-4\\n10. Machine Learning\\n11. Experimental Details\\n12. Passage Ranking\\n13. Evaluation Metrics\\n14. mAP\\n15. nDCG@10\\n16. TREC DL\\n17. Databricks-Dolly-15K\\n18. Self-Reflection\\n19. Information Retrieval\\n20. arXiv Preprint']}\n",
      "\n",
      "\n",
      "Document 53:\n",
      "Content: Metrics Widely-used evaluation metrics for retrieval include mAP, nDCG@10, R@50 and R@1k.\n",
      "Both mAP a...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. Retrieval Evaluation Metrics\\n2. mAP\\n3. nDCG@10\\n4. Recall (R@50', 'R@1k)\\n5. Latency\\n6. Sparse Retrieval\\n7. Dense Retrieval\\n8. BM25\\n9. TF-IDF\\n10. Contriever\\n11. LLM-Embedder\\n12. Pyserini\\n13. Lucene\\n14. MS MARCO\\n15. Faiss\\n16. Flat Configuration\\n17. Query Rewriting\\n18. Zephyr-7b-alpha9\\n19. Query Decomposition\\n20. GPT-3.5-turbo-0125\\n21. HyDE\\n22. GPT-3.5-turbo-instruct\\n23. Temperature\\n24. Token Sampling\\n25. Reranking Methods']}\n",
      "\n",
      "\n",
      "Document 54:\n",
      "Content: tokens. Retrieval experiments and evaluation are conducted using the Pyserini toolkit.\n",
      "A.3 Experimen...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['Here is a list of relevant tags for the provided text:\\n\\n1. Information Retrieval\\n2. Pyserini\\n3. MS MARCO\\n4. Passage Ranking\\n5. Machine Reading Comprehension\\n6. Dataset\\n7. Evaluation Metrics\\n8. MRR (Mean Reciprocal Rank)\\n9. Hit Rate\\n10. Reranking Methods\\n11. PyGaggle\\n12. TILDE\\n13. DLM-based Reranking\\n14. monoT5\\n15. monoBERT\\n16. RankLLaMA\\n17. TILDEv2\\n18. BERT\\n19. T5-base\\n20. Llama-2-7b\\n21. Query\\n22. Document Retrieval\\n23. Relevance Scoring\\n24. Experimental Details']}\n",
      "\n",
      "\n",
      "Document 55:\n",
      "Content: after the reranking and repacking phase can be further concentrated by assigning a top-k value or a\n",
      "...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. Reranking\\n2. Repacking\\n3. Relevancy score\\n4. Top-k value\\n5. Result Analysis\\n6. BM25 baseline\\n7. Retrieval baseline\\n8. Performance metrics\\n9. QA datasets\\n10. MonoT5\\n11. MonoBERT\\n12. RankLLaMA\\n13. TILDEv2\\n14. Latency\\n15. Data comparison\\n16. Hugging Face\\n17. Zephyr-7b-alpha\\n18. Context Model\\n19. NQ (Natural Questions)\\n20. TriviaQA\\n21. HotpotQA\\n22. ASQA (Answer Sentence Quality Assessment)']}\n",
      "\n",
      "\n",
      "Document 56:\n",
      "Content: increase in performance across all metrics. Approximately equal performance is achieved by monoT5\n",
      "an...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. Performance Metrics\\n2. monoT5\\n3. monoBERT\\n4. RankLLaMA\\n5. TILDEv2\\n6. Latency\\n7. Reranking\\n8. Indexing\\n9. Preprocessing\\n10. Summarization Methods\\n11. Selective Context\\n12. Language Models\\n13. Non-query-based Approach\\n14. Dataset Evaluation\\n15. Natural Questions (NQ)\\n16. TriviaQA\\n17. HotpotQA\\n18. F1 Score\\n19. Conciseness\\n20. Llama3-8B-Instruct\\n21. Summarization Ratio\\n22. Extractive Methods\\n23. Abstractive Methods\\n24. Maximum Generation Length']}\n",
      "\n",
      "\n",
      "Document 57:\n",
      "Content: and set a summarization ratio of 0.4. For extractive methods, importance scores determine the\n",
      "senten...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. Summarization\\n2. Extractive methods\\n3. Abstractive methods\\n4. Summarization ratio\\n5. Importance scores\\n6. Question answering (QA)\\n7. Reading comprehension\\n8. Datasets\\n9. Fine-tuning\\n10. Experiments']}\n",
      "\n",
      "\n",
      "Document 58:\n",
      "Content: Table 14: Template for generating task classification data.\n",
      "entries than others, we conducted a rand...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. Task Classification\\n2. Data Generation\\n3. Random Sampling\\n4. Evaluation Metrics\\n5. Datasets\\n6. ASQA\\n7. HotpotQA\\n8. NQ (Natural Questions)\\n9. TriviaQA\\n10. NarrativeQA\\n11. SQuAD\\n12. TruthfulQA\\n13. Fine-Tuning\\n14. Model Training\\n15. Llama-2-7b\\n16. LoRA\\n17. Int8 Quantization\\n18. Prompt Templates\\n19. Ground-Truth Coverage\\n20. Zero-Shot Testing']}\n",
      "\n",
      "\n",
      "Document 59:\n",
      "Content: maximum length of the sequence to 1600, using a batch size of 4 and a learning rate of 5e-5. During\n",
      "...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['1. NLP\\n2. Zero-shot setting\\n3. RAG systems\\n4. Commonsense Reasoning\\n5. Fact Checking\\n6. Open-Domain QA\\n7. MultiHop QA\\n8. Medical QA\\n9. MMLU\\n10. ARC-Challenge\\n11. OpenbookQA\\n12. FEVER\\n13. PubHealth\\n14. NQ\\n15. TriviaQA\\n16. WebQuestions\\n17. HotPotQA\\n18. 2WikiMultiHopQA\\n19. MuSiQue\\n20. PubMedQA\\n21. Token-level F1 score\\n22. EM score\\n23. Batch size\\n24. Learning rate\\n25. Sequence length']}\n",
      "\n",
      "\n",
      "Document 60:\n",
      "Content: 2WikiMultiHopQA and MuSiQue. Each entry is a “question, gold document, gold answer” triple.\n",
      "Metrics ...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['Here is a list of relevant tags for the text:\\n\\n1. 2WikiMultiHopQA\\n2. MuSiQue\\n3. Question Answering (QA)\\n4. MultiHop QA\\n5. Open-Domain QA\\n6. Gold Document\\n7. Gold Answer\\n8. Token-level F1 Score\\n9. Exact Match (EM) Score\\n10. Accuracy\\n11. RAG Capabilities Evaluation\\n12. RAGAs Framework\\n13. Faithfulness\\n14. Context Relevancy\\n15. Answer Relevancy\\n16. Answer Correctness\\n17. GPT-4\\n18. Metrics\\n19. Evaluation\\n20. Model Performance']}\n",
      "\n",
      "\n",
      "Document 61:\n",
      "Content: where |S|denotes the number of relevant sentences, |Total|denotes the total number of sentences\n",
      "retr...\n",
      "Metadata: {'file_name': 'RagOptimization.pdf', 'date': '2024-10-15', 'tags': ['Here is a list of relevant tags for the given text:\\n\\n1. Information Retrieval\\n2. Cosine Similarity\\n3. Open-Domain QA\\n4. MultiHop QA\\n5. NLP (Natural Language Processing)\\n6. RAG (Retrieval-Augmented Generation)\\n7. GPT-4\\n8. RankLLaMA\\n9. LongLLMLingua\\n10. Embedding Models\\n11. Zero-Shot Evaluation\\n12. Greedy Decoding\\n13. Fact Checking\\n14. Multiple Choice\\n15. Natural Language Generation\\n16. Sentence Truncation\\n17. Data Preprocessing\\n18. Regular Expressions']}\n",
      "\n",
      "\n",
      "Result 1:\n",
      "Text: Searching for Best Practices in Retrieval-Augmented\n",
      "Generation\n",
      "Xiaohua Wang, Zhenghua Wang, Xuan Gao, Feiran Zhang,\n",
      "Yixin Wu ,Zhibo Xu ,Tianyuan Shi ,Zhengyuan Wang ,Shizheng Li ,\n",
      "Qi Qian ,Ruicheng Yin ,Changze Lv ,Xiaoqing Zheng∗,Xuanjing Huang\n",
      "School of Computer Science, Fudan University, Shanghai, China\n",
      "Shanghai Key Laboratory of Intelligent Information Processing\n",
      "{xiaohuawang22,zhenghuawang23 }@m.fudan.edu.cn\n",
      "{zhengxq,xjhuang }@fudan.edu.cn\n",
      "Abstract\n",
      "Retrieval-augmented generation (RAG) techniques have proven to be effective\n",
      "in integrating up-to-date information, mitigating hallucinations, and enhancing\n",
      "response quality, particularly in specialized domains. While many RAG approaches\n",
      "have been proposed to enhance large language models through query-dependent\n",
      "retrievals, these approaches still suffer from their complex implementation and\n",
      "prolonged response times. Typically, a RAG workflow involves multiple processing\n",
      "steps, each of which can be executed in various ways. Here, we investigate\n",
      "existing RAG approaches and their potential combinations to identify optimal\n",
      "RAG practices. Through extensive experiments, we suggest several strategies\n",
      "for deploying RAG that balance both performance and efficiency. Moreover,\n",
      "we demonstrate that multimodal retrieval techniques can significantly enhance\n",
      "question-answering capabilities about visual inputs and accelerate the generation\n",
      "of multimodal content using a “retrieval as generation” strategy. Resources are\n",
      "\n",
      "\n",
      "Result 2:\n",
      "Text: •Maintainability : Generation models often necessitate careful fine-tuning to tailor them for new\n",
      "applications. In contrast, retrieval-based methods can be improved to address new demands by\n",
      "simply enlarging the size and enhancing the quality of retrieval sources.\n",
      "We plan to broaden the application of this strategy to include other modalities, such as video and\n",
      "speech, while also exploring efficient and effective cross-modal retrieval techniques.\n",
      "6 Conclusion\n",
      "In this study, we aim to identify optimal practices for implementing retrieval-augmented generation\n",
      "in order to improve the quality and reliability of content produced by large language models. We\n",
      "systematically assessed a range of potential solutions for each module within the RAG framework\n",
      "and recommended the most effective approach for each module. Furthermore, we introduced a\n",
      "comprehensive evaluation benchmark for RAG systems and conducted extensive experiments to\n",
      "determine the best practices among various alternatives. Our findings not only contribute to a deeper\n",
      "understanding of retrieval-augmented generation systems but also establish a foundation for future\n",
      "research.\n",
      "Limitations\n",
      "We have evaluated the impact of various methods for fine-tuning LLM generators. Previous studies\n",
      "have demonstrated the feasibility of training both the retriever and generator jointly. We would\n",
      "like to explore this possibility in the future. In this study, we embraced the principle of modular\n",
      "\n",
      "\n",
      "Result 3:\n",
      "Text: several strategies for deploying RAG that balance both performance and efficiency.\n",
      "The contributions of this study are three-fold:\n",
      "•Through extensive experimentation, we thoroughly investigated existing RAG approaches and their\n",
      "combinations to identify and recommend optimal RAG practices.\n",
      "2•We introduce a comprehensive framework of evaluation metrics and corresponding datasets to\n",
      "comprehensively assess the performance of retrieval-augmented generation models, covering\n",
      "general, specialized (or domain-specific), and RAG-related capabilities.\n",
      "•We demonstrate that the integration of multimodal retrieval techniques can substantially improve\n",
      "question-answering capabilities on visual inputs and speed up the generation of multimodal content\n",
      "through a strategy of “retrieval as generation”.\n",
      "2 Related Work\n",
      "Ensuring the accuracy of responses generated by Large Language Models (LLMs) such as Chat-\n",
      "GPT [ 13] and LLaMA [ 14] is essential. However, simply enlarging model size does not fundamentally\n",
      "address the issue of hallucinations [15, 16], especially in knowledge-intensive tasks and specialized\n",
      "domains. Retrieval-augmented generation (RAG) addresses these challenges by retrieving relevant\n",
      "documents from external knowledge bases, providing accurate, real-time, domain-specific context to\n",
      "LLMs [ 6]. Previous works have optimized the RAG pipeline through query and retrieval transfor-\n",
      "mations, enhancing retriever performance, and fine-tuning both the retriever and generator. These\n",
      "\n",
      "\n",
      "Result 4:\n",
      "Text: Retrieval Capability• \n",
      "• \n",
      "•  \n",
      "Fine -tune\n",
      "Disturb\n",
      "Random\n",
      "Normal• \n",
      "• \n",
      "•    \n",
      "Retrieval SourceFigure 1: Retrieval-augmented generation workflow. This study investigates the contribution of\n",
      "each component and provides insights into optimal RAG practices through extensive experimentation.\n",
      "The optional methods considered for each component are indicated in bold fonts, while the methods\n",
      "underlined indicate the default choice for individual modules. The methods indicated in blue font\n",
      "denote the best-performing selections identified empirically.\n",
      "vector databases to efficiently store feature representations, and the methods for effectively fine-tuning\n",
      "LLMs (see Figure 1).\n",
      "What adds complexity and challenge is the variability in implementing each processing step. For\n",
      "example, in retrieving relevant documents for an input query, various methods can be employed.\n",
      "One approach involves rewriting the query first and using the rewritten queries for retrieval [ 9].\n",
      "Alternatively, pseudo-responses to the query can be generated first, and the similarity between\n",
      "these pseudo-responses and the backend documents can be compared for retrieval [ 10]. Another\n",
      "option is to directly employ embedding models, typically trained in a contrastive manner using\n",
      "positive and negative query-response pairs [ 11,12]. The techniques chosen for each step and their\n",
      "combinations significantly impact both the effectiveness and efficiency of RAG systems. To the best\n",
      "\n",
      "\n",
      "Result 5:\n",
      "Text: question-answering capabilities about visual inputs and accelerate the generation\n",
      "of multimodal content using a “retrieval as generation” strategy. Resources are\n",
      "available at https://github.com/FudanDNN-NLP/RAG .\n",
      "1 Introduction\n",
      "Generative large language models are prone to producing outdated information or fabricating facts,\n",
      "although they were aligned with human preferences by reinforcement learning [ 1] or lightweight\n",
      "alternatives [ 2–5]. Retrieval-augmented generation (RAG) techniques address these issues by com-\n",
      "bining the strengths of pretraining and retrieval-based models, thereby providing a robust framework\n",
      "for enhancing model performance [ 6]. Furthermore, RAG enables rapid deployment of applications\n",
      "for specific organizations and domains without necessitating updates to the model parameters, as\n",
      "long as query-related documents are provided.\n",
      "Many RAG approaches have been proposed to enhance large language models (LLMs) through\n",
      "query-dependent retrievals [ 6–8]. A typical RAG workflow usually contains multiple intervening\n",
      "processing steps: query classification (determining whether retrieval is necessary for a given input\n",
      "query), retrieval (efficiently obtaining relevant documents for the query), reranking (refining the\n",
      "order of retrieved documents based on their relevance to the query), repacking (organizing the\n",
      "retrieved documents into a structured one for better generation), summarization (extracting key\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "import concurrent.futures\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import Document\n",
    "import concurrent.futures\n",
    "import time\n",
    "import PyPDF2\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Assume these functions are already defined:\n",
    "# - extract_text_from_pdf: Extracts text from a PDF file.\n",
    "# - split_text_into_chunks: Splits text into smaller chunks.\n",
    "# - parse_output: Parses the LLM's output to extract the summary, key findings, and tags.\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: Path) -> str:\n",
    "    with pdf_path.open(\"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "    return text\n",
    "\n",
    "def split_text_into_chunks(text, chunk_size=1500, chunk_overlap=200):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "def parse_output(output):\n",
    "    # Print the raw output for debugging\n",
    "    print(\"Raw Output:\\n\", output)\n",
    "    \n",
    "    # Initialize variables\n",
    "    summary, key_findings, tags = \"\", \"\", \"\"\n",
    "    current_section = None\n",
    "    \n",
    "    # Split the output by lines\n",
    "    sections = output.split(\"\\n\")\n",
    "    \n",
    "    # Iterate over each line to determine the current section\n",
    "    for line in sections:\n",
    "        # Normalize the line by removing leading/trailing whitespace and converting to lowercase\n",
    "        normalized_line = line.strip().lower()\n",
    "\n",
    "        # Check for variations of \"summary\" section\n",
    "        if \"summary\" in normalized_line:\n",
    "            current_section = \"summary\"\n",
    "            # Check if there is text after the colon\n",
    "            if \":\" in line:\n",
    "                after_colon = line.split(\":\", 1)[1].strip()\n",
    "                if after_colon:  # If there is text after the colon, add it to the summary\n",
    "                    summary += after_colon + \" \"\n",
    "            continue  # Skip to the next line\n",
    "        \n",
    "        # Check for variations of \"key findings\" section\n",
    "        elif \"key findings\" in normalized_line:\n",
    "            current_section = \"key_findings\"\n",
    "            # Check if there is text after the colon\n",
    "            if \":\" in line:\n",
    "                after_colon = line.split(\":\", 1)[1].strip()\n",
    "                if after_colon:  # If there is text after the colon, add it to the key findings\n",
    "                    key_findings += after_colon + \" \"\n",
    "            continue  # Skip to the next line\n",
    "        \n",
    "        # Check for variations of \"tags\" section\n",
    "        elif \"tags\" in normalized_line:\n",
    "            current_section = \"tags\"\n",
    "            # Check if there is text after the colon\n",
    "            if \":\" in line:\n",
    "                after_colon = line.split(\":\", 1)[1].strip()\n",
    "                if after_colon:  # If there is text after the colon, add it to the tags\n",
    "                    tags += after_colon + \" \"\n",
    "            continue  # Skip to the next line\n",
    "        \n",
    "        # Append content based on the current section\n",
    "        if current_section == \"summary\":\n",
    "            summary += line.strip() + \" \"\n",
    "        elif current_section == \"key_findings\":\n",
    "            key_findings += line.strip() + \" \"\n",
    "        elif current_section == \"tags\":\n",
    "            tags += line.strip() + \" \"\n",
    "    \n",
    "    # Clean and return the parsed results\n",
    "    summary = summary.strip()\n",
    "    key_findings = key_findings.strip()\n",
    "    tags = [tag.strip() for tag in tags.split(\",\") if tag.strip()]\n",
    "\n",
    "    # Return an error if either summary or key findings are empty\n",
    "    if not summary or not key_findings:\n",
    "        raise ValueError(\"Error: Empty summary or key findings for a chunk.\")\n",
    "    \n",
    "    return summary, key_findings, tags\n",
    "\n",
    "# Step 1: Initialize LLM and Prompts\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\")\n",
    "\n",
    "# Create a prompt for summarization and key findings\n",
    "summary_prompt_template = PromptTemplate(\n",
    "    template=(\n",
    "        \"You are an expert analyst. Based on the following text, perform the following tasks:\\n\"\n",
    "        \"1. Summarize the content.\\n\"\n",
    "        \"2. List key findings.\\n\\n\"\n",
    "        \"Text:\\n{text}\"\n",
    "    ),\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Create a prompt for extracting tags\n",
    "tag_prompt_template = PromptTemplate(\n",
    "    template=\"Provide a list of relevant tags for the following text:\\n{text}\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Step 2: Extract text from the PDF\n",
    "pdf_path = Path(\"D:/articles/Rag/RagOptimization.pdf\")\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Step 3: Send entire text to LLM for summarization and key findings\n",
    "summary_chain = LLMChain(llm=llm, prompt=summary_prompt_template)\n",
    "summary_output = summary_chain.run({\"text\": pdf_text})\n",
    "\n",
    "# Parse the LLM output to get the summary and key findings\n",
    "summary, key_findings, _ = parse_output(summary_output)\n",
    "\n",
    "# Print the final summary and key findings\n",
    "print(\"\\nFinal Summary of the Document:\")\n",
    "print(summary)\n",
    "\n",
    "print(\"\\nFinal Key Findings of the Document:\")\n",
    "print(key_findings)\n",
    "\n",
    "# Step 4: Split text into chunks for tag extraction and vector store preparation\n",
    "chunks = split_text_into_chunks(pdf_text)\n",
    "\n",
    "# Create a chain for tag extraction\n",
    "tag_chain = LLMChain(llm=llm, prompt=tag_prompt_template)\n",
    "\n",
    "# Initialize metadata for each chunk\n",
    "file_name = pdf_path.name\n",
    "date = \"2024-10-15\"  # You can dynamically get the current date if needed\n",
    "\n",
    "# Step 5: Process each chunk to extract tags and create documents for vector storage\n",
    "documents = []\n",
    "\n",
    "def process_chunk_for_tags(chunk, file_name, date):\n",
    "    # Get tags for the current chunk\n",
    "    tag_output = tag_chain.run({\"text\": chunk})\n",
    "    time.sleep(0.250)\n",
    "    # Parse the tags and update metadata\n",
    "    tags = [tag.strip() for tag in tag_output.split(\",\") if tag.strip()]\n",
    "    # Create metadata for the document\n",
    "    metadata = {\n",
    "        \"file_name\": file_name,\n",
    "        \"date\": date,\n",
    "        \"tags\": tags\n",
    "    }\n",
    "    # Create a new Document with the updated metadata\n",
    "    document = Document(page_content=chunk, metadata=metadata)\n",
    "    return document\n",
    "\n",
    "# Execute processing in parallel\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    documents = list(executor.map(lambda chunk: process_chunk_for_tags(chunk, file_name, date), chunks))\n",
    "\n",
    "# Step 6: Print documents with updated metadata for verification\n",
    "print(\"\\nProcessed Documents with Updated Metadata:\")\n",
    "for idx, doc in enumerate(documents):\n",
    "    print(f\"Document {idx + 1}:\")\n",
    "    print(f\"Content: {doc.page_content[:100]}...\")  # Show a preview of the content\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Now, `documents` can be stored in the vector store\n",
    "\n",
    "embedding_function = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "texts = [doc.page_content for doc in documents]  # Extract text content\n",
    "metadatas = [doc.metadata for doc in documents]  # Extract metadata\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents, embedding_function)\n",
    "vectorstore.save_local(\"index\")\n",
    "\n",
    "# Later on, you can reload the vector store without needing to re-embed the documents\n",
    "vectorstore = FAISS.load_local(\"index\", embedding_function, allow_dangerous_deserialization=True)\n",
    "\n",
    "\n",
    "query = \"What are best practices for retrieval-augmented generation?\"\n",
    "query_embedding = embedding_function.embed_query(query)\n",
    "\n",
    "retrieved_docs = vectorstore.similarity_search_by_vector(query_embedding, k=5)\n",
    "\n",
    "\n",
    "for idx, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"Result {idx}:\")\n",
    "    print(\"Text:\", doc.page_content)\n",
    "    #print(\"Metadata:\", doc.metadata)\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Combined Topics: ['Retrieval-Augmented Generation (RAG) Techniques', 'Challenges in RAG Implementation', 'Optimal RAG Practices', 'Multimodal Retrieval Techniques', 'Maintainability and Efficiency of RAG Systems', 'Fine-Tuning and Enhancing RAG Systems', 'Evaluation Metrics and Benchmarking', 'Query Classification in RAG Workflows']\n",
      "Before document:\n",
      "# Searching for Best Practices in Retrieval-Augmented Generation\n",
      "\n",
      "## Introduction\n",
      "\n",
      "The rapid evolution of large language models (LLMs) has brought forth challenges in ensuring the accuracy and timeliness of generated responses. These models often produce outdated information or fabricate facts, despite being aligned with human preferences through reinforcement learning or other methods. Retrieval-augmented generation (RAG) techniques provide a powerful solution by integrating the strengths of pretraining and retrieval models, thus offering a robust framework for enhancing model performance. RAG enables rapid deployment of applications for specific organizations and domains without necessitating updates to the model parameters, as long as query-related documents are provided.\n",
      "\n",
      "## Related Work\n",
      "\n",
      "Ensuring the accuracy of responses generated by LLMs, such as ChatGPT and LLaMA, is critical. Simply increasing model size does not address the issue of hallucinations, especially in knowledge-intensive tasks and specialized domains. RAG tackles this by retrieving relevant documents from external knowledge bases, thereby providing accurate, real-time, domain-specific context to LLMs. Previous works have focused on optimizing the RAG pipeline through query and retrieval transformations, enhancing retriever performance, and fine-tuning both the retriever and generator.\n",
      "\n",
      "## RAG Workflow\n",
      "\n",
      "A typical RAG workflow involves multiple processing steps, each executed in various ways:\n",
      "\n",
      "1. **Query Classification**: Determines whether retrieval is necessary for a given input query.\n",
      "2. **Retrieval**: Efficiently obtains relevant documents for the query.\n",
      "3. **Reranking**: Refines the order of retrieved documents based on their relevance to the query.\n",
      "4. **Repacking**: Organizes retrieved documents into a structured format for better generation.\n",
      "5. **Summarization**: Extracts key information from documents.\n",
      "\n",
      "These steps are critical in reducing response time while enhancing accuracy and relevance.\n",
      "\n",
      "## Searching for Best RAG Practices\n",
      "\n",
      "We investigated optimal practices for implementing RAG, using a systematic approach:\n",
      "\n",
      "1. **Query and Retrieval Transformation**: Effective retrieval requires clear and detailed queries. Techniques like Query2Doc and TOC enhance retrieval by generating pseudo-documents or decomposing queries into subqueries.\n",
      "2. **Retriever Enhancement Strategy**: Document chunking and embedding methods significantly impact retrieval performance. Techniques such as LlamaIndex and TILDE enhance retrieval by optimizing chunking and precomputing query term likelihoods.\n",
      "3. **Retriever and Generator Fine-tuning**: Fine-tuning is crucial for optimizing retrievers and generators, ensuring faithful and robust content generation.\n",
      "\n",
      "## Experimental Results\n",
      "\n",
      "Through extensive experiments, we provided insights into the contribution of each component and identified best practices for RAG systems. Key findings include:\n",
      "\n",
      "- **Best Performance Practice**: Incorporates query classification, Hybrid with HyDE for retrieval, monoT5 for reranking, Reverse for repacking, and Recomp for summarization.\n",
      "- **Balanced Efficiency Practice**: Focuses on balancing performance and efficiency by using the Hybrid method for retrieval, TILDEv2 for reranking, and the same choices for repacking and summarization as the best performance practice.\n",
      "\n",
      "## Multimodal Extension\n",
      "\n",
      "We extended RAG to multimodal applications, incorporating text-to-image and image-to-text retrieval capabilities. This multimodal RAG offers advantages such as groundedness and efficiency by providing verified information from multimodal materials.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "In this study, we aimed to identify optimal practices for RAG implementation to improve the quality and reliability of content produced by LLMs. Our findings contribute to a deeper understanding of RAG systems and establish a foundation for future research.\n",
      "\n",
      "## Limitations and Future Work\n",
      "\n",
      "While we evaluated various methods for fine-tuning LLM generators, exploring the joint training of retrievers and generators remains a future goal. Additionally, expanding RAG applications to other modalities such as video and speech is an exciting avenue for future exploration.\n",
      "\n",
      "## Acknowledgments\n",
      "\n",
      "The authors thank the anonymous reviewers for their valuable comments. This work was supported by the National Natural Science Foundation of China (No. 62076068).\n",
      "\n",
      "## References\n",
      "\n",
      "References are available upon request. For resources, visit [FudanDNN-NLP/RAG](https://github.com/FudanDNN-NLP/RAG).\n",
      "\n",
      "---\n",
      "\n",
      "This document outlines the comprehensive research conducted on retrieval-augmented generation, detailing the components, findings, and future directions of the study.\n",
      "After document:\n",
      "# Document: Retrieval-Augmented Generation (RAG) Techniques\n",
      "\n",
      "## Introduction\n",
      "\n",
      "Generative large language models (LLMs) often produce outdated information or fabricate facts. Retrieval-augmented generation (RAG) techniques address these issues by combining the strengths of pretraining and retrieval-based models, providing a robust framework for enhancing model performance. RAG enables rapid deployment of applications in specific domains without needing updates to model parameters, provided that query-related documents are available.\n",
      "\n",
      "Many RAG approaches enhance LLMs through query-dependent retrievals. A typical RAG workflow involves several processing steps: query classification, retrieval, reranking, repacking, and summarization. Each step can be executed in different ways, impacting the effectiveness and efficiency of RAG systems.\n",
      "\n",
      "In this document, we explore existing RAG approaches and their potential combinations to identify optimal practices. Our extensive experimentation suggests strategies for deploying RAG that balance performance and efficiency. We also demonstrate that multimodal retrieval techniques significantly enhance question-answering capabilities for visual inputs, accelerating multimodal content generation through a \"retrieval as generation\" strategy.\n",
      "\n",
      "## Query Classification in RAG Workflows\n",
      "\n",
      "Not all queries require retrieval augmentation due to the inherent capabilities of LLMs. While RAG enhances information accuracy and reduces hallucinations, frequent retrieval can increase response time. Therefore, we classify queries to determine the necessity of retrieval. Queries requiring retrieval proceed through the RAG modules; others are handled directly by LLMs.\n",
      "\n",
      "Retrieval is recommended when knowledge beyond the model’s parameters is needed. However, the necessity varies by task. For instance, an LLM can handle a translation request without retrieval, but an introduction request might require it for relevant information. We propose classifying tasks by type to determine if a query needs retrieval, training a classifier to automate this decision.\n",
      "\n",
      "## RAG Workflow Components\n",
      "\n",
      "The RAG workflow consists of several modules, each contributing uniquely to the system's performance:\n",
      "\n",
      "- **Query Classification**: Enhances accuracy and reduces latency.\n",
      "- **Retrieval**: Efficiently obtains relevant documents for the query.\n",
      "- **Reranking**: Refines the order of retrieved documents based on their relevance.\n",
      "- **Repacking**: Organizes retrieved documents into a structured format for better generation.\n",
      "- **Summarization**: Extracts key passages for the generator.\n",
      "\n",
      "The experimental results demonstrate that each module significantly improves the system’s ability to handle diverse queries, ensuring high-quality responses across different tasks.\n",
      "\n",
      "## Best Practices for Implementing RAG\n",
      "\n",
      "Based on our experimental findings, we suggest two practices for implementing RAG systems:\n",
      "\n",
      "1. **Best Performance Practice**: Incorporate query classification, use the \"Hybrid with HyDE\" method for retrieval, employ monoT5 for reranking, opt for Reverse for repacking, and leverage Recomp for summarization. This configuration yields the highest average score but is computationally intensive.\n",
      "   \n",
      "2. **Balanced Efficiency Practice**: Incorporate the query classification module, implement the Hybrid method for retrieval, use TILDEv2 for reranking, opt for Reverse for repacking, and employ Recomp for summarization. This approach balances performance and efficiency, significantly reducing latency while preserving comparable performance.\n",
      "\n",
      "## Multimodal Retrieval Techniques\n",
      "\n",
      "We have extended RAG to multimodal applications, incorporating text-to-image and image-to-text retrieval capabilities. This extension utilizes a substantial collection of paired images and textual descriptions as a retrieval source. These multimodal RAG capabilities offer advantages like:\n",
      "\n",
      "- **Groundedness**: Retrieval methods provide information from verified multimodal materials, ensuring authenticity and specificity.\n",
      "- **Efficiency**: Retrieval methods are typically more efficient, especially when the answer already exists in stored materials. Generation methods may require more computational resources.\n",
      "\n",
      "## Evaluation Metrics and Benchmarking\n",
      "\n",
      "We introduce a comprehensive framework of evaluation metrics and corresponding datasets to assess the performance of RAG models, covering general, specialized, and RAG-related capabilities. Metrics include Faithfulness, Context Relevancy, Answer Relevancy, and Answer Correctness. We conducted extensive experiments across various NLP tasks and datasets to evaluate RAG systems.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "This study identifies optimal practices for implementing RAG to improve content quality and reliability produced by LLMs. We systematically assessed potential solutions for each module within the RAG framework and recommended the most effective approach for each. Our findings not only contribute to a deeper understanding of RAG systems but also establish a foundation for future research. We aim to explore the possibility of training both the retriever and generator jointly and extend research to other modalities such as speech and video.\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from keybert import KeyBERT\n",
    "from bertopic import BERTopic\n",
    "from sklearn.cluster import KMeans\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "print(load_dotenv(find_dotenv()))\n",
    "embedding_function = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Later on, you can reload the vector store without needing to re-embed the documents\n",
    "vectorstore = FAISS.load_local(\"index\", embedding_function, allow_dangerous_deserialization=True)\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\")\n",
    "\n",
    "query = \"What are best practices for retrieval-augmented generation?\"\n",
    "query_embedding = embedding_function.embed_query(query)\n",
    "\n",
    "retrieved_docs = vectorstore.similarity_search_by_vector(query_embedding, k=20)\n",
    "\n",
    "combined_text = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "\n",
    "\n",
    "structured_topic_prompt_template = PromptTemplate(\n",
    "    template=\"Identify the main topics in the following text and output them in JSON format with fields 'title' and 'description':\\n\\nText:\\n{text}\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Initialize LLM chain with structured prompt\n",
    "structured_topic_chain = LLMChain(llm=llm, prompt=structured_topic_prompt_template)\n",
    "\n",
    "# Run the LLM to get structured JSON output\n",
    "extracted_topics_json = structured_topic_chain.run({\"text\": combined_text})\n",
    "\n",
    "# Display raw JSON output for verification\n",
    "#print(f\"Structured JSON Output:\\n{extracted_topics_json}\")\n",
    "cleaned_json_output = extracted_topics_json.strip(\"```json\").strip(\"```\")\n",
    "#print(f\"Cleaned JSON:\\n{cleaned_json_output}\")\n",
    "import json\n",
    "\n",
    "# Parse JSON output\n",
    "try:\n",
    "    extracted_topics_dict = json.loads(cleaned_json_output)\n",
    "    if isinstance(extracted_topics_dict, list):\n",
    "        # Extract topics assuming it is a list of dictionaries\n",
    "        llm_topics = [(topic.get('title', 'Unknown Title'), topic.get('description', 'No Description')) for topic in extracted_topics_dict]\n",
    "    else:\n",
    "        print(\"Unexpected JSON format. Expected a list of dictionaries.\")\n",
    "        llm_topics = []\n",
    "except json.JSONDecodeError as e:\n",
    "    print(\"Error decoding JSON:\", e)\n",
    "    llm_topics = []\n",
    "\n",
    "\n",
    "# Check if the JSON structure is correct\n",
    "if isinstance(extracted_topics_dict, list):\n",
    "    # Extract topics as a list of dictionaries\n",
    "    llm_topics = [(topic['title'], topic['description']) for topic in extracted_topics_dict if 'title' in topic and 'description' in topic]\n",
    "else:\n",
    "    print(\"Unexpected JSON format. Please check the output.\")\n",
    "\n",
    "# Print parsed topics to verify\n",
    "#print(\"LLM Topics Extracted:\", llm_topics)\n",
    "\n",
    "# Extract only topic titles for vector store searches\n",
    "llm_topic_titles = [topic[0] for topic in llm_topics if topic]  # Ensure no empty tuples are added\n",
    "\n",
    "print(\"Combined Topics:\", llm_topic_titles)\n",
    "\n",
    "\n",
    "\n",
    "def search_vectorstore_for_topic(topic, vectorstore, top_k=10):\n",
    "    \"\"\"\n",
    "    Searches the vector store for the most relevant documents related to a given topic.\n",
    "    \n",
    "    Args:\n",
    "    - topic (str): The topic to search for.\n",
    "    - vectorstore (FAISS): The vector store object.\n",
    "    - top_k (int): The number of top documents to retrieve.\n",
    "\n",
    "    Returns:\n",
    "    - List of Document objects most relevant to the topic.\n",
    "    \"\"\"\n",
    "    # Generate embedding for the topic\n",
    "    topic_embedding = embedding_function.embed_query(topic)\n",
    "    \n",
    "    # Retrieve top_k relevant documents for each topic\n",
    "    retrieved_docs = vectorstore.similarity_search_by_vector(topic_embedding, k=top_k)\n",
    "    return retrieved_docs\n",
    "\n",
    "\n",
    "# Dictionary to hold retrieved documents for each topic\n",
    "topic_documents = {}\n",
    "\n",
    "for topic in llm_topic_titles:\n",
    "    retrieved_docs = search_vectorstore_for_topic(topic, vectorstore)\n",
    "    topic_documents[topic] = retrieved_docs\n",
    " \n",
    " \n",
    "# Initialize a list to gather the final output text\n",
    "final_combined_text = []\n",
    "\n",
    "# Step 1: Add the main retrieved combined text to the final output\n",
    "#final_combined_text.append(combined_text)\n",
    "\n",
    "# Step 2: Add content for each topic in topic_documents\n",
    "for topic, docs in topic_documents.items():\n",
    "    final_combined_text.append(f\"\\n\\n## Topic: {topic}\\n\")\n",
    "    topic_content = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    final_combined_text.append(topic_content)\n",
    "\n",
    "# Combine all parts into a single string\n",
    "final_document = \"\\n\".join(final_combined_text)\n",
    "\n",
    "# Display the final combined document\n",
    "#print(\"Final Combined Document:\")\n",
    "#print(final_document)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "coherence_prompt_template = PromptTemplate(\n",
    "    template=\"Make the following text into a cohesive and well-organized document without removing any details:\\n\\nText:\\n{text}\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "coherence_chain = LLMChain(llm=llm, prompt=coherence_prompt_template)\n",
    "\n",
    "# Run the chain to get a coherent output\n",
    "detailed_document = coherence_chain.run({\"text\": combined_text})\n",
    "\n",
    "# Display the final detailed document\n",
    "print(\"Before document:\")\n",
    "print(detailed_document)\n",
    "\n",
    "detailed_document = coherence_chain.run({\"text\": final_document})\n",
    "\n",
    "# Display the final detailed document\n",
    "print(\"After document:\")\n",
    "print(detailed_document)\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
