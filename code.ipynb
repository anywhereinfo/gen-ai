{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why functions is a List:\n",
    "Multiple Functions Support:\n",
    "\n",
    "The functions list allows you to define multiple functions that the language model can choose from when generating structured responses. Each item in the list is a dictionary describing a function.\n",
    "In example, the list contains only one function definition, but you could add more functions to the list if needed.\n",
    "\n",
    "\n",
    "Function Definition Format:\n",
    "\n",
    "Each function is represented as a dictionary that contains details about the function, such as:\n",
    "- \"name\": The name of the function.\n",
    "- \"description\": A brief description of what the function does.\n",
    "- \"parameters\": The expected parameters for the function, defined using a JSON Schema format.\n",
    "\n",
    "\n",
    "Structured Responses Using Functions:\n",
    "\n",
    "When using functions with the language model, the model can choose to call one of the defined functions by generating a structured response that includes the function name and the associated parameters.\n",
    "\n",
    "Example that using python' OpenAI library to generate a joke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "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\n",
      "No introduction found\n",
      "Why don't scientists trust atoms? Because they make up everything! ...unless they're drunk, then they split and you get a big bang!\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "is_environment_loaded = load_dotenv(find_dotenv())\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "def get_completion_with_function(prompt, model=\"gpt-4o\"):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an assistant that should use functions to provide structured responses whenever available.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    functions = [\n",
    "        {\n",
    "            \"name\": \"tell_joke\",\n",
    "            \"description\": \"Get a joke with an optional introduction\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"introduction\": {\"type\": \"string\", \"description\": \"The introductory phrase for the joke\"},\n",
    "                    \"joke\": {\"type\": \"string\", \"description\": \"The main content of the joke\"}\n",
    "                },\n",
    "                \"required\": [\"joke\"]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        functions=functions\n",
    "    )\n",
    "\n",
    "    #print(\"Raw Response:\", response)\n",
    "    # Access the function_call data from the response\n",
    "    function_call = response.choices[0].message.function_call\n",
    "\n",
    "    if function_call is None:\n",
    "    # If there's no function call in the response, handle it gracefully\n",
    "        return \"No function call was made by the model.\"\n",
    "\n",
    "    # Extract the arguments from the function_call\n",
    "    arguments = function_call.arguments\n",
    "\n",
    "    # Parse the arguments as JSON\n",
    "    import json\n",
    "    parsed_arguments = json.loads(arguments)\n",
    "\n",
    "    # Return the joke\n",
    "    introduction = parsed_arguments.get('introduction')\n",
    "    if introduction is not None:\n",
    "        print(f\"Introduction: {introduction}\")\n",
    "    else:\n",
    "        print(\"No introduction found\")\n",
    "\n",
    "    return parsed_arguments['joke']\n",
    "\n",
    "print(get_completion_with_function(\"tell a drunk joke\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Loaded: True\n",
      "{\n",
      "  \"introduction\": \"Here's a light-hearted joke about a drunk person at a bar.\",\n",
      "  \"joke\": \"A drunk man walks out of a bar, stumbling slightly. He walks up to a cop and says, 'Somebody stole my car!' The cop asks, 'Where was it when you last saw it?' The drunk man replies, 'It was on the end of this key.' The cop chuckles and says, 'Why don't you go down to the station and report it there. But before you go, you might want to zip up your fly.' The man looks down, grins, and says, 'Oh man, they got my girlfriend too!'\"\n",
      "}\n",
      "Result: A drunk man walks out of a bar, stumbling slightly. He walks up to a cop and says, 'Somebody stole my car!' The cop asks, 'Where was it when you last saw it?' The drunk man replies, 'It was on the end of this key.' The cop chuckles and says, 'Why don't you go down to the station and report it there. But before you go, you might want to zip up your fly.' The man looks down, grins, and says, 'Oh man, they got my girlfriend too!'\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "is_environment_loaded = load_dotenv(find_dotenv())\n",
    "print(f\"Environment Loaded: {is_environment_loaded}\")\n",
    "\n",
    "# Set the OpenAI API key\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "client = openai.OpenAI()\n",
    "\n",
    "def get_completion_with_roles(prompt, model=\"gpt-4o\"):\n",
    "    # Use a system role to instruct the model\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": (\n",
    "            \"You are an assistant that provides jokes in a structured format. \"\n",
    "            \"Always put the entire joke content, including both the setup and punchline, in the 'joke' field. \"\n",
    "            \"If there is any optional introductory remark or context before the joke, you can include it in the 'introduction' field, \"\n",
    "            \"but do not separate the question and answer into 'introduction' and 'joke'. \"\n",
    "            \"Respond in the following JSON format:\\n\"\n",
    "            \"{\\n\"\n",
    "            '  \"introduction\": \"Optional introductory remarks before the joke (if any)\",\\n'\n",
    "            '  \"joke\": \"The entire joke content, including setup and punchline\"\\n'\n",
    "            \"}\"\n",
    "        )},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    # Create chat completion request\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "        # Extract the content of the response\n",
    "    content = response.choices[0].message.content.strip()\n",
    "    print(content)\n",
    "    import json\n",
    "    # Return the response content\n",
    "        # Parse the content as JSON\n",
    "    try:\n",
    "        parsed_content = json.loads(content)\n",
    "        # Return just the joke\n",
    "        return parsed_content.get(\"joke\", \"Joke not found.\")\n",
    "    except json.JSONDecodeError:\n",
    "        return \"Failed to parse the response as JSON.\"\n",
    "\n",
    "result = get_completion_with_roles(\"Tell a drunk joke\")\n",
    "print(\"Result:\", result)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
